---
# Uses this html: "_layout/post.html" as a template.
layout: post 
title: Noisy Tensors & Machine Learning I & II
# (Optional) Used to sort posts, not supported as of writing this.
categories: [ Workshops ]
---

![Tensors and Operators](/uploads/images/Moore.png)


<a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">
<img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" />
Cris Moore</a>

<a name="Moore" />
<b>Title:Noisy Tensors and Machine Learning (tutorial)</b>

**Speaker:** <a href="http://tuvalu.santafe.edu/~moore/" target="_blank">Cris Moore</a>, Santa Fe Institute

**Abstract:** Someone gives you a tensor. You have a hunch that it is a noisy observation of an underlying tensor which is low rank. Depending on what kind of noise has been applied, can you recover the underlying matrix, at least approximately? Can you do this efficiently? And how much noise can you tolerate? And what does this have to do with statistical physics?


---

 {% 
    include video.html
    src="https://vimeo.com/389585458"
    title="Noisy Tensors & Machine Learning I -- Cris Moore"
    desc="TACA 2019. For more visit https://TheTensor.Space/. Creative Commons 2.0 CC-ND 2019 Cris Moore."
  %}


The second lecture will be about what tensors can tell us about computational complexity. We will see several ways - both direct and indirect - in which problems on tensors lie at the heart of lower bounds in complexity theory.

{% 
    include video.html
    src="https://vimeo.com/389585673"
    title="Noisy Tensors & Machine Learning II -- Cris Moore"
    desc="TACA 2019. For more visit https://TheTensor.Space/. Creative Commons 2.0 CC-ND 2019 Joshua Grochow."
  %}


<br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.
